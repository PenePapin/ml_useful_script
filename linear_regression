import kagglehub
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
from sklearn.dummy import DummyRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR

# Download latest version
path = kagglehub.dataset_download("abhilashanil/california-housing-prices")
print("Path to dataset files:", path)

df = pd.read_csv( path + '/housing.csv' )

cols_numeric = [col for col in df.columns if col not in ['ocean_proximity']]
df[cols_numeric] = df[cols_numeric].fillna( df[cols_numeric].median() )
sns.scatterplot( 
    x='longitude', 
    y='latitude', 
    data=df,
    hue='median_house_value',
    alpha=0.4,
    palette='coolwarm'
    )
# plt.colorbar('Median House Value')
plt.show()
# Feature engineering
big_city_pos = [
    (-118.244, 34.052),  # Los Angeles
    (-117.165, 32.716),  # San Diego
    (-121.895, 37.339),  # San Jose
    (-122.419, 37.775),  # San Francisco
    (-119.772, 36.748),  # Fresno
    (-121.494, 38.582),  # Sacramento
    (-118.189, 33.767),  # Long Beach
    (-122.271, 37.804),  # Oakland
    (-119.019, 35.373),  # Bakersfield
    (-117.915, 33.835)   # Anaheim
]

def dist_to_big_city( longitude, latitude ):
    return min( ( ( longitude - x ) ** 2 + ( latitude - y ) ** 2 ) ** ( 1/2 ) for x, y in big_city_pos )
df['dist_to_big_city'] = df.apply(lambda row: dist_to_big_city(row['longitude'], row['latitude']), axis=1)
df['is_near_big_city'] = df['dist_to_big_city'] < 0.5
df['rooms_per_household'] = df['total_rooms'] / df['households']
# df['log_rooms_per_household'] = np.log( df['rooms_per_household'] )
df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']
# df['log_bedrooms_per_room'] = np.log(df['bedrooms_per_room'])
df['persons_per_room'] = df['population'] / df['total_rooms']
# df['log_persons_per_room'] = np.log(df['persons_per_room'])
df = df[[col for col in df.columns if col not in ['longitude', 'latitude']]]

print( df.columns )

cols_categoric = [ 'ocean_proximity', 'is_near_big_city' ]
cols_numeric = [ col for col in df.columns if col not in cols_categoric + ['median_house_value'] ]
sns.heatmap( df[cols_numeric + ['median_house_value']].corr() )
print( df[cols_numeric + ['median_house_value']].corr()['median_house_value'].sort_values(ascending=False) )

# Prepare data
X = df.drop('median_house_value', axis=1)
y = df['median_house_value']

# Preprocessing for numeric and categorical data
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(drop='first', handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, cols_numeric),
        ('cat', categorical_transformer, cols_categoric)
    ])

# Define models
models = {
    'Benchmark': DummyRegressor(strategy='mean'),
    'Linear': LinearRegression(),
    'Ridge': RidgeCV(alphas=np.logspace(-6, 6, 13)),
    'Lasso': LassoCV(alphas=np.logspace(-6, 6, 13), max_iter=10000),
    # 'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42),
    'SVR': SVR()
}

# Perform 5-fold cross validation for each model
for name, model in models.items():
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])
    # Using neg_mean_squared_error for scoring so that higher is better
    neg_mse_scores = cross_val_score(
        pipeline, X, y, cv=5, scoring='neg_mean_squared_error'
    )
    mse_scores = -neg_mse_scores
    print(f"{name} CV RMSE: {np.sqrt(mse_scores).mean():.3f} (+/- {np.sqrt(mse_scores).std():.3f})")

# Fine tune the hyperparameters of the chosen models. GridSearchCV, RandomizedSearchCV, etc...
# ...

# Train the selected model on X..
