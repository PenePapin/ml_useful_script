import kagglehub
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Download latest version
path = kagglehub.dataset_download("abhilashanil/california-housing-prices")
print("Path to dataset files:", path)

df = pd.read_csv( path + '/housing.csv' )

cols_numeric = [col for col in df.columns if col not in ['ocean_proximity']]
df[cols_numeric] = df[cols_numeric].fillna( df[cols_numeric].median() )
sns.scatterplot( 
    x='longitude', 
    y='latitude', 
    data=df,
    hue='median_house_value',
    alpha=0.4,
    palette='coolwarm'
    )
# plt.colorbar('Median House Value')
plt.show()
# Feature engineering
big_city_pos = [
    (-118.244, 34.052),  # Los Angeles
    (-117.165, 32.716),  # San Diego
    (-121.895, 37.339),  # San Jose
    (-122.419, 37.775),  # San Francisco
    (-119.772, 36.748),  # Fresno
    (-121.494, 38.582),  # Sacramento
    (-118.189, 33.767),  # Long Beach
    (-122.271, 37.804),  # Oakland
    (-119.019, 35.373),  # Bakersfield
    (-117.915, 33.835)   # Anaheim
]

def dist_to_big_city( longitude, latitude ):
    return min( ( ( longitude - x ) ** 2 + ( latitude - y ) ** 2 ) ** ( 1/2 ) for x, y in big_city_pos )
df['dist_to_big_city'] = df.apply(lambda row: dist_to_big_city(row['longitude'], row['latitude']), axis=1)
df['is_near_big_city'] = df['dist_to_big_city'] < 0.5
df['rooms_per_household'] = df['total_rooms'] / df['households']
df['log_rooms_per_household'] = np.log( df['rooms_per_household'] )
df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']
df['log_bedrooms_per_room'] = np.log(df['bedrooms_per_room'])
df['persons_per_room'] = df['population'] / df['total_rooms']
df['log_persons_per_room'] = np.log(df['persons_per_room'])
df = df[[col for col in df.columns if col not in ['longitude', 'latitude']]]

print( df.columns )

cols_categoric = [ 'ocean_proximity', 'is_near_big_city' ]
cols_numeric = [ col for col in df.columns if col not in cols_categoric + ['median_house_value'] ]
sns.heatmap( df[cols_numeric + ['median_house_value']].corr() )
print( df[cols_numeric + ['median_house_value']].corr()['median_house_value'].sort_values(ascending=False) )


# Model selections

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
from sklearn.dummy import DummyRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR

# Prepare data
X = df.drop('median_house_value', axis=1)
y = df['median_house_value']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing for numeric and categorical data
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, cols_numeric),
        ('cat', categorical_transformer, cols_categoric)
    ])

# Compare models
models = {
    'Benchmark': DummyRegressor(strategy='mean'),
    'Linear': LinearRegression(),
    'Ridge': RidgeCV(alphas=np.logspace(-6, 6, 13)),
    'Lasso': LassoCV(alphas=np.logspace(-6, 6, 13), max_iter=10000),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=42),
    'SVR': SVR()
}

for name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', model)])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    print(f"{name} RMSE: {mean_squared_error(y_test, y_pred, squared=False)}")


# Cross Validation for selected models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
from sklearn.dummy import DummyRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR

# Prepare data
X = df.drop('median_house_value', axis=1)
y = df['median_house_value']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing for numeric and categorical data
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, cols_numeric),
        ('cat', categorical_transformer, cols_categoric)
    ])

# Compare models
models = {
    'Benchmark': DummyRegressor(strategy='mean'),
    'Linear': LinearRegression(),
    'Ridge': RidgeCV(alphas=np.logspace(-6, 6, 13)),
    'Lasso': LassoCV(alphas=np.logspace(-6, 6, 13), max_iter=10000),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=42),
    'SVR': SVR()
}

for name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', model)])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    print(f"{name} RMSE: {mean_squared_error(y_test, y_pred, squared=False)}")

